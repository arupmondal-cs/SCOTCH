{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL0CEEAbi0as"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVB3_nv8swyt"
      },
      "source": [
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "from numpy import dstack \n",
        "import pandas as pd\n",
        "from keras.layers import (Conv2D, Dense, Dropout, Flatten, GaussianNoise,\n",
        "                          MaxPooling2D, MaxPool2D , Activation)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot\n",
        "from numpy import dstack, mean, std\n",
        "from pandas import read_csv\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPI2dHOtrkml"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHqWCUqp9Gc2"
      },
      "source": [
        "#FedAvg using MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3W--962kVpS"
      },
      "source": [
        "### For N clients FedAvG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjbY9Mqn9Qs5"
      },
      "source": [
        "def avgWeights(scaledWeights):\n",
        "    avg = list()\n",
        "    for weight_list_tuple in zip(*scaledWeights):\n",
        "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
        "        avg.append(layer_mean)\n",
        "    return avg\n",
        "\n",
        "def FedAvg(models):  \n",
        "    scaledWeights = []\n",
        "    for m in models:\n",
        "        scaledWeights.append(getScaledWeight(m, FLAccuracyDF.loc[m]['Weightage']))\n",
        "    avgWeight = avgWeights(scaledWeights)\n",
        "    return avgWeigh\n",
        "\n",
        "def define_mod2(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(350, input_shape=input_shape, activation='relu'))\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # Configure the model and start training\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAelalCOrj59",
        "outputId": "652061cf-8085-4566-aa50-280dfc6a0e12"
      },
      "source": [
        "import numpy as np\n",
        "feature_vector_length = 784\n",
        "# Set the input shape\n",
        "input_shape = (feature_vector_length,)\n",
        "print(f'Feature shape: {input_shape}')\n",
        "\n",
        "# Load the data\n",
        "def load_dataset2(n_clients=3,permute=False):\n",
        "\n",
        "  client_datasets = {} # defining local datasets for each client\n",
        "\n",
        "  (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        " \n",
        "  X_train = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
        "  X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
        "\n",
        "  if permute==True:\n",
        "    permutation_indexes = np.random.permutation(len(X_train))\n",
        "    X_train  = X_train[permutation_indexes]\n",
        "    Y_train = Y_train[permutation_indexes]\n",
        "\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_train /= 255\n",
        "  X_test /= 255\n",
        "\n",
        "  # Convert target classes to categorical ones\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_test = to_categorical(Y_test)\n",
        "\n",
        "  for i in range(n_clients):\n",
        "    client_datasets[i] = [X_train[i*(len(X_train)//n_clients):i*(len(X_train)//n_clients)+(len(X_train)//n_clients)],Y_train[i*(len(Y_train)//n_clients):i*(len(Y_train)//n_clients)+(len(Y_train)//n_clients)]]\n",
        "\n",
        "\n",
        "  return client_datasets  \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature shape: (784,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pngP_86SmNq8"
      },
      "source": [
        "def load_dataset3(n_clients=3,permute=False):\n",
        "\n",
        "  client_datasets = {} # defining local datasets for each client\n",
        "\n",
        "  (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        " \n",
        "  X_train = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
        "  X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
        "\n",
        "  if permute==True:\n",
        "    permutation_indexes = np.random.permutation(len(X_train)//n_clients)\n",
        "    X_train  = X_train[permutation_indexes]\n",
        "    Y_train = Y_train[permutation_indexes]\n",
        "\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_train /= 255\n",
        "  X_test /= 255\n",
        "\n",
        "  # Convert target classes to categorical ones\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_test = to_categorical(Y_test)\n",
        "\n",
        "  return X_train,X_test,Y_train,Y_test\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPTGGhbjU5sA"
      },
      "source": [
        "def testing():\n",
        "\n",
        "  (_, _), (X_test, Y_test) = mnist.load_data() \n",
        "  X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_test /= 255\n",
        "  # Convert target classes to categorical ones\n",
        "  Y_test = to_categorical(Y_test)\n",
        "  return X_test,Y_test  \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXRRSFFmyEis"
      },
      "source": [
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# Decimal-Integer Conversion\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "def f_to_i(x,scale = 1<<32):\n",
        "  if x < 0:\n",
        "    if pow(2,64) - (abs(x)*(scale)) > (pow(2,64) - 1):\n",
        "      return np.uint64(0)\n",
        "    x = pow(2,64) - np.uint64(abs(x)*(scale))\n",
        "   \n",
        "  else:\n",
        "    x = np.uint64(scale*x)\n",
        "  \n",
        "  return np.uint64(x)\n",
        "\n",
        "\t\t\t\t\n",
        "\n",
        "\n",
        "def i_to_f(x,scale = 1<<32):\n",
        "  l=64\n",
        "  t = x > ( pow(2,(l-1)) -1 )\n",
        "  if t:\n",
        "    x = pow(2,l) - x\n",
        "    y = np.uint64(x)\n",
        "    y = np.float32(y*(-1))/scale\n",
        "\n",
        "  else:\n",
        "    y = np.float32(np.uint64(x))/scale\n",
        "    \n",
        "\n",
        "  return y\n",
        "\n",
        "\n",
        "f_to_i_v = np.vectorize(f_to_i)\n",
        "i_to_f_v = np.vectorize(i_to_f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCj4liAFhSPi"
      },
      "source": [
        "\n",
        "def fed_avg_pipe(N_clients=10,num_of_iterations=3,num_servers=3):\n",
        "  client_datasets = load_dataset2(N_clients,permute=True)\n",
        "  LD = len(client_datasets[0][0])//num_of_iterations\n",
        "\n",
        "  X_test,Y_test = testing()\n",
        "  avg_wts=0\n",
        "  for iteration in range(num_of_iterations):\n",
        "    list_of_weights = []\n",
        "    for j in range(N_clients):\n",
        "      if iteration==0:\n",
        "        model_c = define_mod2(input_shape)\n",
        "      else:\n",
        "        model_c = define_mod2(input_shape)\n",
        "        model_c.set_weights(avg_wts)\n",
        "\n",
        "      X_train,Y_train = client_datasets[j][0][(iteration*LD) :(iteration*LD)+LD],client_datasets[j][1][(iteration*LD) :(iteration*LD)+LD]\n",
        "\n",
        "      model_c.fit(X_train, Y_train, epochs=4, batch_size=100, verbose=0, validation_split=0.1)\n",
        "\n",
        "      client_models[j]= model_c\n",
        "      W_c= model_c.get_weights()\n",
        "      list_of_weights.append(W_c)\n",
        "     \n",
        "    ################## Simulating Secret Sharing Framework Setting ####################################\n",
        "\n",
        "    final_weights=[None]*len(list_of_weights[0])\n",
        "    all_servers=[{}]*num_servers\n",
        "    for m in range(N_clients):\n",
        "      layer_dict,layer_shape,shares_dict={},{},{}\n",
        "      data=list_of_weights[m]\n",
        "      no_of_layers=len(data)\n",
        "      for i in range(len(data)):\n",
        "          layer_dict[i]=data[i]\n",
        "          layer_shape[i]=data[i].shape\n",
        "          \n",
        "      for i in range(no_of_layers):\n",
        "          shares_dict[i]=np.random.randint(1000,size=(num_servers,)+layer_shape[i],dtype=np.uint64)\n",
        "          x = f_to_i_v(layer_dict[i])\n",
        "          for k in range(0,num_servers-1):\n",
        "              shares_dict[i][k]=np.random.randint(1000,size=layer_shape[i],dtype=np.uint64)\n",
        "              x = x - shares_dict[i][k]\n",
        "          shares_dict[i][num_servers-1] = x\n",
        "                              \n",
        "      for i in range(num_servers):\n",
        "        if m==0:\n",
        "          for j in range(len(shares_dict)):\n",
        "            shape=shares_dict[j][0].shape\n",
        "            all_servers[i][j]=np.random.random_sample((N_clients,)+shape)\n",
        "            all_servers[i][j][m]=shares_dict[j][0] \n",
        "        else:\n",
        "          for j in range(len(shares_dict)):\n",
        "            all_servers[i][j][m]=shares_dict[j][i]\n",
        "\n",
        "\n",
        "    for i in range(num_servers):\n",
        "      for j in range(len(all_servers[0])):\n",
        "        all_servers[i][j] = f_to_i_v(i_to_f_v(all_servers[i][j])/np.float32(N_clients))\n",
        "  \n",
        "    for i in range(num_servers):\n",
        "      for j in range(len(all_servers[0])):\n",
        "        for k in range(N_clients):\n",
        "          if i==0 and k==0:\n",
        "            final_weights[j]=all_servers[i][j][k]\n",
        "          else:\n",
        "            final_weights[j]=final_weights[j]+all_servers[i][j][k]\n",
        "\n",
        "    for j in range(len(final_weights)):\n",
        "      final_weights[j]=i_to_f_v(final_weights[j])\n",
        "    \n",
        "    ################## Simulating Secret Sharing Framework Setting ####################################\n",
        "\n",
        "    avg_wts=final_weights\n",
        "    avg_model=define_mod2(input_shape)\n",
        "    avg_model.set_weights(avg_wts)\n",
        "\n",
        "  test_results = avg_model.evaluate(X_test, Y_test, verbose=1)\n",
        "  return test_results[1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0e6rr0MiTzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71154569-305d-47b8-c8cd-10099ddd8283"
      },
      "source": [
        "test_accuracies = []\n",
        "for y in range(4,5,1):\n",
        "  for q in range(2,5,1):\n",
        "    res = fed_avg_pipe(N_clients=q,num_of_iterations=y,num_servers=3)\n",
        "    test_accuracies.append(res)\n",
        "    #print(f\" {q} :  {res}\")\n",
        "  print(f\"\\n\\n\\n With {y} iterations the test_acc is {test_accuracies}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.8859 - accuracy: 0.9257\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.2158 - accuracy: 0.8484\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.2811 - accuracy: 0.7497\n",
            "\n",
            "\n",
            "\n",
            " With 4 iterations the test_acc is [0.9347000122070312, 0.8633999824523926, 0.7736999988555908]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz5EUlSV_GFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27650c8e-4c77-453a-9689-2daae2d6b92c"
      },
      "source": [
        "print(test_accuracies)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9347000122070312, 0.8633999824523926, 0.7736999988555908]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}